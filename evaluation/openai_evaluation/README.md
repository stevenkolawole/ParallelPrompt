# Overview

This repository contains two Python scripts designed to evaluate and analyze responses generated by LLMs using another LLM as a judge. The scripts facilitate a comparative analysis between two sets of LLM-generated responses (e.g., “serial” and “parallel”) based on specific criteria such as accuracy, grammar, detail, and overall preference.

# Scripts

## `openai_evaluation.py`

### Description

This script evaluates pairs of LLM-generated responses using another LLM as a judge. It compares the responses based on
- Accuracy: How well the response follows the prompt instructions.
- Grammar: The grammatical correctness of the response.
- Detail: The level of detail and specificity in the response.
- Preference: The judge’s overall preference considering all factors.
- Reasoning: A brief explanation supporting the evaluations.

### How It Works

- Input: A JSON file containing prompts and two sets of responses for each prompt (e.g., “serial” and “parallel”).
-	Process:
	-	Shuffles the order of the responses to avoid bias.
    -	Sends the prompt and responses to an LLM (`GPT-4o`) for evaluation.
	-	Parses the LLM’s JSON-formatted evaluation.
    -	Maps the evaluations back to the original response labels.
-	Output: A JSON file containing the evaluations for each prompt.

### Command-Line Arguments

- `--input`: (Required) Path to the input JSON file containing prompts and responses.
- `--output`: (Optional) Path to the output JSON file to save evaluation results. Default is `evaluation_results.json`.
- `--model`: (Optional) OpenAI model to use for evaluation. Default is `gpt-4o`.

### Input File Format

The input JSON file should be a list of dictionaries, each containing:
- `"prompt"`: The text of the prompt.
- `"serial_output"`: The response from the serial execution.
- `"parallel_output"`: A list of responses from the parallel execution.

Example:
```json
[
  {
    "prompt": "Explain the theory of relativity.",
    "serial_output": "The theory of relativity was developed by Albert Einstein...",
    "parallel_output": [
      "Albert Einstein's theory of relativity revolutionized physics...",
      "In the early 20th century, Einstein proposed the theory of relativity..."
    ]
  },
  ...
]
```

Usage example:
```bash
python openai_evaluation.py --input path_to_input.json --output path_to_output.json
```

## `parse_openai_evaluation.py`

Purpose

This script parses the evaluation results produced by `openai_evaluation.py` and computes statistics on how often each set of responses (serial vs. parallel) was preferred based on each evaluation criterion.

### How It Works

- Input: A JSON file containing the evaluation results.
- Process:
	- Reads the evaluations and tallies the number of times each response set won, lost, or tied in each category.
- Output: Prints the statistics to the console.

Command-Line Arguments

- `--input`: (Required) Path to the evaluation results JSON file.

### Usage example

```bash
python parse_openai_evaluation.py --input path_to_evaluation_results.json
```

### Output example

```
Evaluation Statistics:

Accuracy Results:
Serial wins: 10
Parallel wins: 15
Ties: 5

Grammar Results:
Serial wins: 12
Parallel wins: 13
Ties: 5

Detail Results:
Serial wins: 8
Parallel wins: 17
Ties: 5

Preference Results:
Serial wins: 9
Parallel wins: 16
Ties: 5
```
